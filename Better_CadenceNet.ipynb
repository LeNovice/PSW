{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeNovice/PSW/blob/main/Better_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGru5NMgz0L"
      },
      "source": [
        "Implementing\n",
        "\n",
        "Focal Loss:\n",
        "* https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/\n",
        "* https://github.com/artemmavrin/focal-loss\n",
        "\n",
        "If we make Data Augmentation as a layer in the model (maybe only for training) then we won't see overfitting on training or validation data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCeppY1fBRsz"
      },
      "outputs": [],
      "source": [
        "USE_ORIGINAL = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IYvbodAaO5NT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model building\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.utils as tfutils\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bJSJfW_tPA88"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "DataSet = 'caltech101'\n",
        "#'cifar10'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:(NUM_CLASSES + 1)]    #+1 because we are going to remove \"backround_google\" i.e. 4\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2fLIbNS2PDZ1"
      },
      "outputs": [],
      "source": [
        "#ds_train = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True)\n",
        "ds_train, train_info = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True, with_info = True)\n",
        "ds_test = tfds.load(DataSet, split='test', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "QOoGq7JtPGn8",
        "outputId": "2ff3f861-05d7-4e88-b33e-afb7ddd0ccea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['airplanes',\n",
              " 'bonsai',\n",
              " 'car_side',\n",
              " 'chandelier',\n",
              " 'faces',\n",
              " 'faces_easy',\n",
              " 'hawksbill',\n",
              " 'leopards',\n",
              " 'motorbikes',\n",
              " 'watch']"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "if DataSet == 'caltech101':\n",
        "  class_list = [i for i in class_list if i != train_info.features['label'].str2int('background_google')]\n",
        "  class_list.sort()\n",
        "\n",
        "\"\"\"for name in train_info.features['label'].names:\n",
        "    print(name, train_info.features['label'].str2int(name))\n",
        "\"\"\"\n",
        "\n",
        "class_names = [train_info.features['label'].int2str(i) for i in class_list]\n",
        "display(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VeThcLypHU4m"
      },
      "outputs": [],
      "source": [
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApyIbKhPISb",
        "outputId": "ec6b1bb8-fbb7-41c6-c510-bbe079f4b310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 611\n",
            "9 100\n",
            "16 102\n",
            "20 85\n",
            "37 339\n",
            "38 331\n",
            "46 86\n",
            "57 154\n",
            "66 627\n",
            "95 196\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8z2tJJ3PLsE",
        "outputId": "69bd47fb-2b4d-42b0-b343-1a8bc4cbae6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 770\n",
            "9 98\n",
            "16 93\n",
            "20 77\n",
            "37 405\n",
            "38 405\n",
            "46 70\n",
            "57 170\n",
            "66 768\n",
            "95 209\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QdPKLGVdPNrk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "IMG_SIZE = 60\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n",
        "\n",
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)\n",
        "#Relabelling to avoid issues. Note that human readability is reduced by this\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        #values=tf.constant([tfutils.to_categorical(0, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(1, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(2, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(3, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(4, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(5, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(6, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(7, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(8, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(9, num_classes=NUM_CLASSES, dtype=np.int64)],  dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64)\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "#This function will be used in the graph execution hence @tf.function prefix\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    mapped_label = table.lookup(label)\n",
        "    #print(type(mapped_label))\n",
        "    #mapped_label = tf.keras.utils.to_categorical(tf.make_ndarray(mapped_label), num_classes=NUM_CLASSES)\n",
        "    mapped_label = tf.one_hot(indices=mapped_label, depth=NUM_CLASSES)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "#Preprocessing done as part of the graph\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#Preprocessing function which invokes above graphs\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        #f_ds = ds.filter(lambda x, y: filter_fn(y, [2,3,6]))    #[2,3,6] are the examples with lesser data. We are trying to bring back balance\n",
        "        #f_ds_aug = f_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(f_ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmLFCHD6PgLw",
        "outputId": "9131a9e7-1e30-4867-9d4f-212c98e2adc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"one_hot:0\", shape=(10,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "eBn0xUIRPizz",
        "outputId": "89e6f37c-3ac8-487d-ca62-7ebd4c1bd0df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=8>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for example in resized_ds_train.take(1):\n",
        "  plt.imshow(example[0])\n",
        "  display((example[-1]))\n",
        "  display(tf.argmax(example[-1]))\n",
        "  #display(train_info.features['label'].int2str(example[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_samples_per_class_onehot(resized_ds_train, print_all=False):\n",
        "    vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: tf.argmax(y)), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    class_hist.sort()\n",
        "    return class_hist\n",
        "\n"
      ],
      "metadata": {
        "id": "uf1KScmu9odE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "avZ2eG0Ty3fi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "aac5372b-f47e-41f1-a8c7-2c42a89c71a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[(0, 2444),\n",
              " (1, 400),\n",
              " (2, 408),\n",
              " (3, 340),\n",
              " (4, 1356),\n",
              " (5, 1324),\n",
              " (6, 344),\n",
              " (7, 616),\n",
              " (8, 2508),\n",
              " (9, 784)]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Post prepare function, all the labels will be converted to one hot encoders. In order to get class-wise distribution, we will need to convert each one hot encoder into its label (temporarily)\n",
        "#We need a new function to handle it\n",
        "class_hist = num_samples_per_class_onehot(resized_ds_train)\n",
        "display(class_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftnZ5OyvQB98",
        "outputId": "37d5d20c-8764-4847-a973-159fc92c8f0b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Better CadenceNet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#reg = tf.keras.regularizers.L2(0.01)\n",
        "#reg = tf.keras.regularizers.L1L2(l1 =0.01, l2 = 0.1)\n",
        "reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.0)\n",
        "#beta_regularizer = 0.1\n",
        "#gamma_regularizer = 0.1\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "if USE_ORIGINAL == 1:\n",
        "\t\tdisplay(\"Original CadenceNet\")\n",
        "\t\tmodel.add(resize_and_rescale)\n",
        "\t\tmodel.add(data_augmentation)\n",
        "\t\tkernel_size = (5,5)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())                                                      #beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())\n",
        "else:\n",
        "\t\tdisplay(\"Better CadenceNet\")\n",
        "\t\tmodel.add(resize_and_rescale)\n",
        "\t\tmodel.add(data_augmentation)\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\t#kernel_size = (3,3)\n",
        "\t\t#model.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\t#model.add(layers.BatchNormalization())\n",
        "\t\t#model.add(layers.ReLU())\n",
        "\t\t#pool_size = (2,2)\n",
        "\t\t#model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\t#model.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(1000, kernel_regularizer = reg))\n",
        "\t\t#model.add(layers.Dropout(.02))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B-aANPwedhNH"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(class_hist):\n",
        "    \"\"\"\n",
        "    Returns the class weights as a tf.Tensor. Class weights are inverse of the class frequencies\n",
        "    Class frequencies are the number of samples of each class which we calculate in earlier steps\n",
        "    \"\"\"\n",
        "    inv_freq = tf.convert_to_tensor([1.0/count for label, count in class_hist], dtype=tf.float32)\n",
        "    return tfutils.normalize(inv_freq)\n",
        "\n",
        "\n",
        "def weightedloss(y_true, y_pred, gamma, class_weight):\n",
        "    \"\"\"\n",
        "    We assume that all arguments coming into this function are tf.Tensors type\n",
        "    class_weights are basically alpha in focal loss paper\n",
        "    \"\"\"\n",
        "    #ones = tf.convert_to_tensor(np.ones(shape=len(y_true)))\n",
        "    a = tf.math.multiply(tf.math.pow(tf.math.subtract(1.0, y_pred), gamma), tf.math.log(y_pred))  #((1-pt)^gamma)log(pt)\n",
        "    b = tf.math.multiply(-1.0, class_weight)                                                          #-alpha\n",
        "    b = tf.math.multiply(b,a)    \n",
        "    b = tf.math.multiply(b, y_true)\n",
        "    return b\n",
        "class WeightedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma, class_weight=np.ones(shape=NUM_CLASSES, dtype=np.float32)):\n",
        "        super().__init__()\n",
        "        self.gamma = tf.convert_to_tensor(gamma)\n",
        "        self.class_weight = tf.convert_to_tensor(class_weight, dtype=tf.float32)\n",
        "    def call(self, y_true, y_pred):\n",
        "        return weightedloss(y_true, y_pred, self.gamma, self.class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Learning_Rate = 1e-5\n",
        "#tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.0)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)"
      ],
      "metadata": {
        "id": "fHRlWtV83IeV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DHFaNgqtwZGz"
      },
      "outputs": [],
      "source": [
        "###EITHER\n",
        "\n",
        "#!pip install focal-loss\n",
        "#from focal_loss import SparseCategoricalFocalLoss \n",
        "#model.compile( optimizer = opt, loss = SparseCategoricalFocalLoss(gamma=2), metrics=['accuracy'] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "IpzOv6ADQP8L",
        "outputId": "57f08523-43a8-416b-ef48-55a267768733"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.06910105, 0.4222074 , 0.41392884, 0.4967146 , 0.12454496,\n",
              "        0.1275551 , 0.49093884, 0.27416065, 0.06733771, 0.21541193]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "###OR\n",
        "\n",
        "model.compile( optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'] )\n",
        "#model.compile( optimizer = opt, loss = 'categorical_crossentropy', metrics=['categorical_accuracy'] )\n",
        "class_wts = get_class_weights(class_hist)\n",
        "display(class_wts)\n",
        "#model.compile( optimizer = opt, loss = WeightedLoss(gamma=0.0), metrics=['accuracy'] )\n",
        "\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKoB4BcEQVkU",
        "outputId": "67d76dad-2187-492e-8734-a974bedb7198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     83/Unknown - 25s 248ms/step - loss: 2.1174 - accuracy: 0.2368"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r83/83 [==============================] - 27s 273ms/step - loss: 2.1174 - accuracy: 0.2368 - val_loss: 2.3682 - val_accuracy: 0.0228\n",
            "Epoch 2/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.6742 - accuracy: 0.3874 - val_loss: 2.3965 - val_accuracy: 0.0313\n",
            "Epoch 3/20\n",
            "83/83 [==============================] - 22s 268ms/step - loss: 1.6157 - accuracy: 0.4244 - val_loss: 2.4054 - val_accuracy: 0.0303\n",
            "Epoch 4/20\n",
            "83/83 [==============================] - 23s 278ms/step - loss: 1.5830 - accuracy: 0.4417 - val_loss: 2.3990 - val_accuracy: 0.0303\n",
            "Epoch 5/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.5650 - accuracy: 0.4438 - val_loss: 2.3476 - val_accuracy: 0.1064\n",
            "Epoch 6/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.5455 - accuracy: 0.4500 - val_loss: 2.1808 - val_accuracy: 0.2604\n",
            "Epoch 7/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.5420 - accuracy: 0.4522 - val_loss: 1.9092 - val_accuracy: 0.3892\n",
            "Epoch 8/20\n",
            "83/83 [==============================] - 22s 264ms/step - loss: 1.5242 - accuracy: 0.4552 - val_loss: 1.6719 - val_accuracy: 0.4049\n",
            "Epoch 9/20\n",
            "83/83 [==============================] - 22s 267ms/step - loss: 1.5173 - accuracy: 0.4573 - val_loss: 1.5854 - val_accuracy: 0.4062\n",
            "Epoch 10/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.5157 - accuracy: 0.4586 - val_loss: 1.6302 - val_accuracy: 0.3605\n",
            "Epoch 11/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.5010 - accuracy: 0.4664 - val_loss: 1.6051 - val_accuracy: 0.4212\n",
            "Epoch 12/20\n",
            "83/83 [==============================] - 22s 262ms/step - loss: 1.5021 - accuracy: 0.4669 - val_loss: 1.5353 - val_accuracy: 0.4450\n",
            "Epoch 13/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.4924 - accuracy: 0.4685 - val_loss: 1.5488 - val_accuracy: 0.4241\n",
            "Epoch 14/20\n",
            "83/83 [==============================] - 23s 273ms/step - loss: 1.4889 - accuracy: 0.4702 - val_loss: 1.5585 - val_accuracy: 0.4192\n",
            "Epoch 15/20\n",
            "83/83 [==============================] - 24s 286ms/step - loss: 1.4815 - accuracy: 0.4751 - val_loss: 1.6465 - val_accuracy: 0.3993\n",
            "Epoch 16/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.4754 - accuracy: 0.4763 - val_loss: 1.5320 - val_accuracy: 0.4157\n",
            "Epoch 17/20\n",
            "83/83 [==============================] - 22s 265ms/step - loss: 1.4697 - accuracy: 0.4774 - val_loss: 1.5918 - val_accuracy: 0.3843\n",
            "Epoch 18/20\n",
            "83/83 [==============================] - 23s 271ms/step - loss: 1.4689 - accuracy: 0.4808 - val_loss: 1.5158 - val_accuracy: 0.4623\n",
            "Epoch 19/20\n",
            "83/83 [==============================] - 23s 276ms/step - loss: 1.4694 - accuracy: 0.4786 - val_loss: 1.4657 - val_accuracy: 0.4675\n",
            "Epoch 20/20\n",
            "83/83 [==============================] - 23s 278ms/step - loss: 1.4649 - accuracy: 0.4812 - val_loss: 1.4766 - val_accuracy: 0.4845\n"
          ]
        }
      ],
      "source": [
        "#h = model.fit( resized_ds_train, epochs=10)\n",
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_test_unbatched = resized_ds_test\n",
        "resized_ds_test = resized_ds_test.batch(BATCH_SIZE)\n",
        "\n",
        "h = model.fit( resized_ds_train, epochs=20, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FuOyiBsTQkYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5f7ef0b1-9198-4d06-eac1-124ab7e59428"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Z3R0lTfUlo"
      },
      "source": [
        "How to get the X and Y labels as numpy arrays from tfds:\n",
        "\n",
        "https://stackoverflow.com/questions/72763708/how-to-get-the-x-and-y-as-numpy-array-from-a-tensorflow-prefetch-tf-data-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pxSSK5SPhnKL"
      },
      "outputs": [],
      "source": [
        "#Evaluation and confusion matrix creation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "x_test = np.asarray(list(map(lambda x: x[0], tfds.as_numpy(resized_ds_test_unbatched))))\n",
        "y_test_orig = np.asarray(list(map(lambda x: x[1], tfds.as_numpy(resized_ds_test_unbatched))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tg4EdPBuc7fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc408ad7-66fb-4d4c-baf8-0c967a5c8577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 1s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sYYhORws1NnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d61a051e-fd17-4a8a-d83f-fc8a95de0836"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3065, 10)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "false_arr = np.full(shape=len(class_list), fill_value = False)\n",
        "#y_pred = np.empty(shape=y_test_orig.shape[-1])\n",
        "i=0\n",
        "for i, pred in enumerate(predictions):\n",
        "    temp_arr = copy.deepcopy(false_arr)\n",
        "    np.put(temp_arr, np.argmax(pred), True)\n",
        "    if i==0:\n",
        "        y_pred = copy.deepcopy(temp_arr)\n",
        "    else:\n",
        "        y_pred = np.vstack([y_pred, temp_arr])\n",
        "display(y_pred.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Go8FAKdprJVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "f6d8c1a7-e391-41db-ba99-58bdba5b5fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[455,  10,   0,   4, 117,  15,   0,   0, 151,  18],\n",
              "       [  9,   8,   0,   3,  40,   5,   0,   0,   8,  25],\n",
              "       [ 19,   6,   0,   0,  55,   2,   0,   0,   0,  11],\n",
              "       [  3,   7,   0,   7,  22,   6,   0,   0,   1,  31],\n",
              "       [ 54,  10,   0,   9, 224,  50,   2,   0,   0,  56],\n",
              "       [ 79,  16,   0,   8, 205,  71,   0,   0,   2,  24],\n",
              "       [  4,   1,   0,   3,  39,  11,   0,   0,   1,  11],\n",
              "       [  8,   1,   0,   3, 133,  11,   0,   0,   0,  14],\n",
              "       [ 87,   1,   0,   0,  22,   4,   0,   0, 609,  45],\n",
              "       [  9,  17,   0,   9,  30,  12,   0,   0,  21, 111]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   airplanes       0.63      0.59      0.61       770\n",
            "      bonsai       0.10      0.08      0.09        98\n",
            "    car_side       0.00      0.00      0.00        93\n",
            "  chandelier       0.15      0.09      0.11        77\n",
            "       faces       0.25      0.55      0.35       405\n",
            "  faces_easy       0.38      0.18      0.24       405\n",
            "   hawksbill       0.00      0.00      0.00        70\n",
            "    leopards       0.00      0.00      0.00       170\n",
            "  motorbikes       0.77      0.79      0.78       768\n",
            "       watch       0.32      0.53      0.40       209\n",
            "\n",
            "   micro avg       0.48      0.48      0.48      3065\n",
            "   macro avg       0.26      0.28      0.26      3065\n",
            "weighted avg       0.46      0.48      0.46      3065\n",
            " samples avg       0.48      0.48      0.48      3065\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print('Confusion Matrix')\n",
        "matrix = confusion_matrix(y_test_orig.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "display(matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test_orig, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FRx9tVhibX"
      },
      "source": [
        "NOT using below things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NaFdDuTQoyT"
      },
      "outputs": [],
      "source": [
        "def ret_as_numpy():\n",
        "    test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    test = prepare(test)\n",
        "    test = tfds.as_numpy(test)\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_MguzMQuZL"
      },
      "outputs": [],
      "source": [
        "test_as_np = ret_as_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWYWlODgQrFy"
      },
      "outputs": [],
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        #display(output[0])\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    #display(output[0])\n",
        "    #display(output)\n",
        "    #display(digit)\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    #display(prediction_digits)\n",
        "    #display(test_labels)\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHIU_J3QxE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03000f0-1f75-422a-a27b-3847c04fa799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "\n",
            "\n",
            "Float test_accuracy: 0.5046022353714661\n"
          ]
        }
      ],
      "source": [
        "test_accuracy_Float = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy_Float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q2Is9IY-Oo"
      },
      "source": [
        "Float checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fr6QAx7Qztb",
        "outputId": "f12b8152-ed9a-40f9-a72e-b11ebd0d7c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/238.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfG_qAU4Q3DL",
        "outputId": "a85b5a60-408f-40ec-f34c-19a0d48664a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 60, 60, 3)        3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 60, 60, 64)       4993      \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_batch_normalization (  (None, 60, 60, 64)       257       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_re_lu (QuantizeWrappe  (None, 60, 60, 64)       3         \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 30, 30, 64)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWrap  (None, 30, 30, 64)       1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 30, 30, 192)      111169    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_1  (None, 30, 30, 192)      769       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_1 (QuantizeWrap  (None, 30, 30, 192)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 15, 15, 192)      1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_1 (QuantizeWr  (None, 15, 15, 192)      1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 15, 15, 64)       110785    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_2  (None, 15, 15, 64)       257       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_2 (QuantizeWrap  (None, 15, 15, 64)       3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_2 (QuantizeWr  (None, 15, 15, 64)       1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 15, 15, 128)      74113     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_3  (None, 15, 15, 128)      513       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_3 (QuantizeWrap  (None, 15, 15, 128)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Quan  (None, 7, 7, 128)        1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_3 (QuantizeWr  (None, 7, 7, 128)        1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 6272)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_4 (QuantizeWr  (None, 6272)             1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               62735     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_softmax (QuantizeWrap  (None, 10)               1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 365,616\n",
            "Trainable params: 363,786\n",
            "Non-trainable params: 1,830\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "q_aware_model = quantize_model(model)\n",
        "q_aware_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYdW-VK8Q5mT"
      },
      "outputs": [],
      "source": [
        "quantize_train, quant_train_info = tfds.load(DataSet, split='train + test[:75%]', with_info=True, as_supervised=True)\n",
        "filtered_quantize_train = quantize_train.filter(lambda x, y: filter_fn(y, class_list))\n",
        "\n",
        "resized_quantize_train = prepare(filtered_quantize_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VQBS_obQ8DF",
        "outputId": "50be512f-52b8-4215-fe78-c9a43adb06b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "21/21 [==============================] - 11s 452ms/step - loss: 93.8971 - accuracy: 0.6421 - val_loss: 92.0019 - val_accuracy: 0.7373\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 5s 263ms/step - loss: 90.5158 - accuracy: 0.6736 - val_loss: 89.0145 - val_accuracy: 0.7463\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 6s 278ms/step - loss: 87.7361 - accuracy: 0.7003 - val_loss: 86.6357 - val_accuracy: 0.7483\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 5s 261ms/step - loss: 85.5085 - accuracy: 0.7067 - val_loss: 84.6382 - val_accuracy: 0.7421\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 5s 258ms/step - loss: 83.5398 - accuracy: 0.7307 - val_loss: 82.8535 - val_accuracy: 0.7334\n"
          ]
        }
      ],
      "source": [
        "resized_quantize_train = resized_quantize_train.batch(BATCH_SIZE)\n",
        "h = q_aware_model.fit(resized_quantize_train, epochs=5, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pHskuvDaRBPb",
        "outputId": "cba28a60-0b0c-4423-9c51-1c305ba7498b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFq4nw4PRDYT",
        "outputId": "66f445e2-0e16-4d64-be4b-782e60dd01b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbUyvBrRGBD"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ol3wMK2RIjN",
        "outputId": "3fe65086-275b-4d4d-bd7b-5052bff1aab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.48520710059171596\n"
          ]
        }
      ],
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVX3TFb5RKeD",
        "outputId": "9256d51d-166c-44a0-91c9-60f6c3cce62f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKEqxRdmRMOL",
        "outputId": "8341415f-5ae8-42e3-c992-341a04d73a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf2onnx==1.8.4\n",
            "  Downloading tf2onnx-1.8.4-py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m345.3/345.3 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (2.25.1)\n",
            "Collecting onnx>=1.4.1\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.4.1->tf2onnx==1.8.4) (4.4.0)\n",
            "Collecting protobuf<4,>=3.20.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (2.10)\n",
            "Installing collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.13.0 protobuf-3.20.3 tf2onnx-1.8.4\n",
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-02-07 09:59:43,332 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-02-07 09:59:44,202 - INFO - Signatures found in model: [serving_default].\n",
            "2023-02-07 09:59:44,202 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-02-07 09:59:44,202 - INFO - Output names: ['softmax']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2023-02-07 09:59:44,333 - WARNING - From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2023-02-07 09:59:44,378 - INFO - Using tensorflow=2.9.2, onnx=1.13.0, tf2onnx=1.8.4/cd55bf\n",
            "2023-02-07 09:59:44,378 - INFO - Using opset <onnx, 9>\n",
            "2023-02-07 09:59:44,413 - INFO - Computed 0 values for constant folding\n",
            "2023-02-07 09:59:44,553 - INFO - Optimizing ONNX model\n",
            "2023-02-07 09:59:44,604 - INFO - After optimization: BatchNormalization -4 (4->0), Cast -1 (1->0), Const -16 (27->11), Identity -10 (10->0), Transpose -20 (22->2)\n",
            "2023-02-07 09:59:44,607 - INFO - \n",
            "2023-02-07 09:59:44,607 - INFO - Successfully converted TensorFlow model /content/CadenceNet_Float/ to ONNX\n",
            "2023-02-07 09:59:44,607 - INFO - Model inputs: ['conv2d_input:0']\n",
            "2023-02-07 09:59:44,607 - INFO - Model outputs: ['softmax']\n",
            "2023-02-07 09:59:44,607 - INFO - ONNX model is saved at /content/CadenceNetOriginal_Float.onnx\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tf2onnx==1.8.4\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNetOriginal_Float.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLDe9u9sRORk",
        "outputId": "becd55e3-6ce1-4cb1-9ef8-f3451c0c29ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "381680"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "quant_file = \"/content/CadenceNetOriginal_QAT.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5iITOiiRP0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216a1801-c12f-40dc-ae18-f1030e8506d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model in Mb:  1.3894729614257812\n",
            "Quantized model in Mb:  0.3639984130859375\n",
            "Float Model Accuracy:  0.5046022353714661\n",
            "Quantized Model Accuracy:  0.48520710059171596\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Float model in Mb: \", os.path.getsize(\"/content/CadenceNetOriginal_Float.onnx\") / float(2**20))\n",
        "print(\"Quantized model in Mb: \", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float Model Accuracy: \", test_accuracy_Float)\n",
        "print(\"Quantized Model Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime\n",
        "import onnxruntime as rt\n",
        "\n",
        "sess = rt.InferenceSession(\"/content/CadenceNetOriginal_Float.onnx\")\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "x = np.random.random((1,IMG_SIZE,IMG_SIZE,NUM_CHANNELS))\n",
        "x = x.astype(np.float32)\n",
        "res = sess.run([output_name], {input_name: x})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOfhD35IW0f",
        "outputId": "9273a3e7-b262-4d29-b9d3-7b6f53041154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_byw6-0Um7c"
      },
      "outputs": [],
      "source": [
        "indices = tf.convert_to_tensor([0, 1, 2])\n",
        "depth = 3\n",
        "indic = tf.convert_to_tensor([3, 5, 8])\n",
        "tf.math.multiply(indices, indic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ftfq2j6b0CrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}