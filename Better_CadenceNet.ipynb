{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeNovice/PSW/blob/main/Better_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGru5NMgz0L"
      },
      "source": [
        "Implementing\n",
        "\n",
        "Focal Loss:\n",
        "* https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/\n",
        "* https://github.com/artemmavrin/focal-loss\n",
        "\n",
        "If we make Data Augmentation as a layer in the model (maybe only for training) then we won't see overfitting on training or validation data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCeppY1fBRsz"
      },
      "outputs": [],
      "source": [
        "USE_ORIGINAL = 0\n",
        "loss = 'sparse_categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYvbodAaO5NT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model building\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.utils as tfutils\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJSJfW_tPA88"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "DataSet = 'caltech101'\n",
        "#'cifar10'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:(NUM_CLASSES + 1)]    #+1 because we are going to remove \"backround_google\" i.e. 4\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fLIbNS2PDZ1"
      },
      "outputs": [],
      "source": [
        "#ds_train = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True)\n",
        "ds_train, train_info = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True, with_info = True)\n",
        "ds_test = tfds.load(DataSet, split='test', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "QOoGq7JtPGn8",
        "outputId": "11a0f98b-9b4b-492f-9485-f83c1aabb0e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['airplanes',\n",
              " 'bonsai',\n",
              " 'car_side',\n",
              " 'faces',\n",
              " 'faces_easy',\n",
              " 'hawksbill',\n",
              " 'ketch',\n",
              " 'leopards',\n",
              " 'motorbikes',\n",
              " 'watch']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "if DataSet == 'caltech101':\n",
        "  class_list = [i for i in class_list if i != train_info.features['label'].str2int('background_google')]\n",
        "  class_list.sort()\n",
        "\n",
        "\"\"\"for name in train_info.features['label'].names:\n",
        "    print(name, train_info.features['label'].str2int(name))\n",
        "\"\"\"\n",
        "\n",
        "class_names = [train_info.features['label'].int2str(i) for i in class_list]\n",
        "display(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeThcLypHU4m"
      },
      "outputs": [],
      "source": [
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApyIbKhPISb",
        "outputId": "10c46aec-66ae-4f82-b1a9-f54659ad82ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 610\n",
            "9 103\n",
            "16 101\n",
            "37 337\n",
            "38 333\n",
            "46 87\n",
            "54 88\n",
            "57 155\n",
            "66 626\n",
            "95 194\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8z2tJJ3PLsE",
        "outputId": "e474bd39-0cc2-4d74-8422-53b03c0ce9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 770\n",
            "9 98\n",
            "16 93\n",
            "37 405\n",
            "38 405\n",
            "46 70\n",
            "54 84\n",
            "57 170\n",
            "66 768\n",
            "95 209\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdPKLGVdPNrk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "IMG_SIZE = 60\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n",
        "\n",
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)\n",
        "#Relabelling to avoid issues. Note that human readability is reduced by this\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        #values=tf.constant([tfutils.to_categorical(0, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(1, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(2, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(3, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(4, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(5, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(6, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(7, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(8, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(9, num_classes=NUM_CLASSES, dtype=np.int64)],  dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64)\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "#This function will be used in the graph execution hence @tf.function prefix\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    global loss\n",
        "    mapped_label = table.lookup(label)\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        mapped_label = tf.one_hot(indices=mapped_label, depth=NUM_CLASSES)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "#Preprocessing done as part of the graph\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#Preprocessing function which invokes above graphs\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        #f_ds = ds.filter(lambda x, y: filter_fn(y, [2,3,6]))    #[2,3,6] are the examples with lesser data. We are trying to bring back balance\n",
        "        #f_ds_aug = f_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(f_ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmLFCHD6PgLw",
        "outputId": "f4db1347-aafb-4e93-db31-18797a4c3393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"None_Lookup/LookupTableFindV2:0\", shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBn0xUIRPizz"
      },
      "outputs": [],
      "source": [
        "for example in resized_ds_train.take(1):\n",
        "  plt.imshow(example[0])\n",
        "  display((example[-1]))\n",
        "  display(tf.argmax(example[-1]))\n",
        "  #display(train_info.features['label'].int2str(example[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf1KScmu9odE"
      },
      "outputs": [],
      "source": [
        "def num_samples_per_class_onehot(resized_ds_train, print_all=False):\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: tf.argmax(y)), int), return_counts=True)\n",
        "    else:\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    class_hist.sort()\n",
        "    return class_hist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "avZ2eG0Ty3fi",
        "outputId": "0ce92202-a9ad-4218-9e48-a24e5c629327"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 2440),\n",
              " (1, 412),\n",
              " (2, 404),\n",
              " (3, 1348),\n",
              " (4, 1332),\n",
              " (5, 348),\n",
              " (6, 352),\n",
              " (7, 620),\n",
              " (8, 2504),\n",
              " (9, 776)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Post prepare function, all the labels will be converted to one hot encoders. In order to get class-wise distribution, we will need to convert each one hot encoder into its label (temporarily)\n",
        "#We need a new function to handle it\n",
        "class_hist = num_samples_per_class_onehot(resized_ds_train)\n",
        "display(class_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftnZ5OyvQB98",
        "outputId": "57c35e65-f5be-403b-9f96-02c925dad349"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Better CadenceNet'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#reg = tf.keras.regularizers.L2(0.01)\n",
        "#reg = tf.keras.regularizers.L1L2(l1 =0.01, l2 = 0.1)\n",
        "reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.0)\n",
        "#beta_regularizer = 0.1\n",
        "#gamma_regularizer = 0.1\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "if USE_ORIGINAL == 1:\n",
        "\t\tdisplay(\"Original CadenceNet\")\n",
        "\t\tmodel.add(resize_and_rescale)\n",
        "\t\tmodel.add(data_augmentation)\n",
        "\t\tkernel_size = (5,5)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())                                                      #beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())\n",
        "else:\n",
        "\t\tdisplay(\"Better CadenceNet\")\n",
        "\t\tmodel.add(resize_and_rescale)\n",
        "\t\tmodel.add(data_augmentation)\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\t#kernel_size = (3,3)\n",
        "\t\t#model.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\t#model.add(layers.BatchNormalization())\n",
        "\t\t#model.add(layers.ReLU())\n",
        "\t\t#pool_size = (2,2)\n",
        "\t\t#model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\t#model.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(1000, kernel_regularizer = reg))\n",
        "\t\t#model.add(layers.Dropout(.02))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-aANPwedhNH"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(class_hist):\n",
        "    \"\"\"\n",
        "    Returns the class weights as a tf.Tensor. Class weights are inverse of the class frequencies\n",
        "    Class frequencies are the number of samples of each class which we calculate in earlier steps\n",
        "    \"\"\"\n",
        "    inv_freq = tf.convert_to_tensor([1.0/count for label, count in class_hist], dtype=tf.float32)\n",
        "    return tfutils.normalize(inv_freq)\n",
        "\n",
        "\n",
        "def weightedloss(y_true, y_pred, gamma, class_weight):\n",
        "    \"\"\"\n",
        "    We assume that all arguments coming into this function are tf.Tensors type\n",
        "    class_weights are basically alpha in focal loss paper\n",
        "    \"\"\"\n",
        "    #ones = tf.convert_to_tensor(np.ones(shape=len(y_true)))\n",
        "    a = tf.math.multiply(tf.math.pow(tf.math.subtract(1.0, y_pred), gamma), tf.math.log(y_pred))  #((1-pt)^gamma)log(pt)\n",
        "    b = tf.math.multiply(-1.0, class_weight)                                                          #-alpha\n",
        "    b = tf.math.multiply(b,a)    \n",
        "    b = tf.math.multiply(b, y_true)\n",
        "    return b\n",
        "class WeightedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma, class_weight=np.ones(shape=NUM_CLASSES, dtype=np.float32)):\n",
        "        super().__init__()\n",
        "        self.gamma = tf.convert_to_tensor(gamma)\n",
        "        self.class_weight = tf.convert_to_tensor(class_weight, dtype=tf.float32)\n",
        "    def call(self, y_true, y_pred):\n",
        "        return weightedloss(y_true, y_pred, self.gamma, self.class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHRlWtV83IeV"
      },
      "outputs": [],
      "source": [
        "Learning_Rate = 1e-5\n",
        "#tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.0)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHFaNgqtwZGz",
        "outputId": "84cf0c6e-8983-49f5-d74b-2ffa4d4fc7ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: focal-loss in /usr/local/lib/python3.8/dist-packages (0.0.7)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.8/dist-packages (from focal-loss) (2.9.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.9.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.12)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (15.0.6.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.19.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.9.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.51.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (4.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (2.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2->focal-loss) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "###EITHER\n",
        "\n",
        "!pip install focal-loss\n",
        "from focal_loss import SparseCategoricalFocalLoss \n",
        "model.compile( optimizer = opt, loss = SparseCategoricalFocalLoss(gamma=2), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFFBTJNwLAcA"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "#model.compile( optimizer = opt, loss = loss, metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmqZtduVbl-2"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "#class_wts = get_class_weights(class_hist)\n",
        "#display(class_wts)\n",
        "#model.compile( optimizer = opt, loss = WeightedLoss(gamma=2.0, class_weight=class_wts), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL7YFZKvbn92"
      },
      "outputs": [],
      "source": [
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKoB4BcEQVkU",
        "outputId": "3d6a6151-1427-46d5-a760-1a026fab2c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     83/Unknown - 38s 397ms/step - loss: 1.5358 - accuracy: 0.2879"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83/83 [==============================] - 42s 443ms/step - loss: 1.5358 - accuracy: 0.2879 - val_loss: 1.8727 - val_accuracy: 0.2500\n",
            "Epoch 2/20\n",
            "83/83 [==============================] - 23s 276ms/step - loss: 1.2824 - accuracy: 0.3853 - val_loss: 1.8553 - val_accuracy: 0.2500\n",
            "Epoch 3/20\n",
            "83/83 [==============================] - 22s 270ms/step - loss: 1.2105 - accuracy: 0.4167 - val_loss: 1.8372 - val_accuracy: 0.2500\n",
            "Epoch 4/20\n",
            "83/83 [==============================] - 24s 291ms/step - loss: 1.1775 - accuracy: 0.4319 - val_loss: 1.8076 - val_accuracy: 0.0680\n",
            "Epoch 5/20\n",
            "83/83 [==============================] - 23s 283ms/step - loss: 1.1618 - accuracy: 0.4437 - val_loss: 1.7474 - val_accuracy: 0.0791\n",
            "Epoch 6/20\n",
            "83/83 [==============================] - 23s 278ms/step - loss: 1.1456 - accuracy: 0.4435 - val_loss: 1.6431 - val_accuracy: 0.2858\n",
            "Epoch 7/20\n",
            "83/83 [==============================] - 23s 278ms/step - loss: 1.1327 - accuracy: 0.4496 - val_loss: 1.3920 - val_accuracy: 0.3564\n",
            "Epoch 8/20\n",
            "83/83 [==============================] - 24s 287ms/step - loss: 1.1200 - accuracy: 0.4500 - val_loss: 1.1894 - val_accuracy: 0.4274\n",
            "Epoch 9/20\n",
            "83/83 [==============================] - 22s 272ms/step - loss: 1.1094 - accuracy: 0.4504 - val_loss: 1.1046 - val_accuracy: 0.4391\n",
            "Epoch 10/20\n",
            "83/83 [==============================] - 23s 277ms/step - loss: 1.1039 - accuracy: 0.4581 - val_loss: 1.1147 - val_accuracy: 0.4577\n",
            "Epoch 11/20\n",
            "83/83 [==============================] - 23s 280ms/step - loss: 1.0959 - accuracy: 0.4623 - val_loss: 1.1876 - val_accuracy: 0.3958\n",
            "Epoch 12/20\n",
            "83/83 [==============================] - 23s 279ms/step - loss: 1.0892 - accuracy: 0.4604 - val_loss: 1.1849 - val_accuracy: 0.4076\n",
            "Epoch 13/20\n",
            "83/83 [==============================] - 23s 278ms/step - loss: 1.0866 - accuracy: 0.4626 - val_loss: 1.1435 - val_accuracy: 0.4333\n",
            "Epoch 14/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.0785 - accuracy: 0.4651 - val_loss: 1.0936 - val_accuracy: 0.4743\n",
            "Epoch 15/20\n",
            "83/83 [==============================] - 23s 278ms/step - loss: 1.0755 - accuracy: 0.4709 - val_loss: 1.1364 - val_accuracy: 0.4264\n",
            "Epoch 16/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.0694 - accuracy: 0.4755 - val_loss: 1.2707 - val_accuracy: 0.3675\n",
            "Epoch 17/20\n",
            "83/83 [==============================] - 23s 284ms/step - loss: 1.0654 - accuracy: 0.4750 - val_loss: 1.1449 - val_accuracy: 0.4720\n",
            "Epoch 18/20\n",
            "83/83 [==============================] - 22s 266ms/step - loss: 1.0613 - accuracy: 0.4788 - val_loss: 1.1904 - val_accuracy: 0.4137\n",
            "Epoch 19/20\n",
            "83/83 [==============================] - 22s 261ms/step - loss: 1.0622 - accuracy: 0.4754 - val_loss: 1.3285 - val_accuracy: 0.4023\n",
            "Epoch 20/20\n",
            "83/83 [==============================] - 21s 258ms/step - loss: 1.0550 - accuracy: 0.4782 - val_loss: 1.2336 - val_accuracy: 0.4229\n"
          ]
        }
      ],
      "source": [
        "#h = model.fit( resized_ds_train, epochs=10)\n",
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_test_unbatched = resized_ds_test\n",
        "resized_ds_test = resized_ds_test.batch(BATCH_SIZE)\n",
        "\n",
        "h = model.fit( resized_ds_train, epochs=20, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FuOyiBsTQkYL",
        "outputId": "504b35fa-469a-40a9-d5fe-d37330e923e8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pxSSK5SPhnKL"
      },
      "outputs": [],
      "source": [
        "#Evaluation and confusion matrix creation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "x_test = np.asarray(list(map(lambda x: x[0], tfds.as_numpy(resized_ds_test_unbatched))))\n",
        "y_test_orig = np.asarray(list(map(lambda x: x[1], tfds.as_numpy(resized_ds_test_unbatched))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tg4EdPBuc7fW",
        "outputId": "bf38359d-53f5-4bf2-cec1-5fbfbd08f1d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (32, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (32, 60, 60, 3).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 1s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYYhORws1NnZ"
      },
      "outputs": [],
      "source": [
        "if loss!='sparse_categorical_crossentropy':\n",
        "    false_arr = np.full(shape=len(class_list), fill_value = False)\n",
        "    #y_pred = np.empty(shape=y_test_orig.shape[-1])\n",
        "    i=0\n",
        "    for i, pred in enumerate(predictions):\n",
        "        temp_arr = copy.deepcopy(false_arr)\n",
        "        np.put(temp_arr, np.argmax(pred), True)\n",
        "    if i==0:\n",
        "        y_pred = copy.deepcopy(temp_arr)\n",
        "    else:\n",
        "        y_pred = np.vstack([y_pred, temp_arr])\n",
        "    display(y_pred.shape)\n",
        "else:\n",
        "    y_pred = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-iQ19WTaE9s"
      },
      "outputs": [],
      "source": [
        "display(y_test_orig.shape)\n",
        "display(y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go8FAKdprJVD"
      },
      "outputs": [],
      "source": [
        "print('Confusion Matrix')\n",
        "if loss != 'sparse_categorical_crossentropy':\n",
        "    matrix = confusion_matrix(y_test_orig.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "else:\n",
        "    matrix = confusion_matrix(y_test_orig, y_pred)\n",
        "display(matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test_orig, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FRx9tVhibX"
      },
      "source": [
        "NOT using below things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NaFdDuTQoyT"
      },
      "outputs": [],
      "source": [
        "def ret_as_numpy():\n",
        "    test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    test = prepare(test)\n",
        "    test = tfds.as_numpy(test)\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_MguzMQuZL"
      },
      "outputs": [],
      "source": [
        "test_as_np = ret_as_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWYWlODgQrFy"
      },
      "outputs": [],
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        #display(output[0])\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    #display(output[0])\n",
        "    #display(output)\n",
        "    #display(digit)\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    #display(prediction_digits)\n",
        "    #display(test_labels)\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHIU_J3QxE7"
      },
      "outputs": [],
      "source": [
        "test_accuracy_Float = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy_Float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q2Is9IY-Oo"
      },
      "source": [
        "Float checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fr6QAx7Qztb",
        "outputId": "f12b8152-ed9a-40f9-a72e-b11ebd0d7c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/238.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfG_qAU4Q3DL",
        "outputId": "a85b5a60-408f-40ec-f34c-19a0d48664a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 60, 60, 3)        3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 60, 60, 64)       4993      \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_batch_normalization (  (None, 60, 60, 64)       257       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_re_lu (QuantizeWrappe  (None, 60, 60, 64)       3         \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 30, 30, 64)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWrap  (None, 30, 30, 64)       1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 30, 30, 192)      111169    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_1  (None, 30, 30, 192)      769       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_1 (QuantizeWrap  (None, 30, 30, 192)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 15, 15, 192)      1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_1 (QuantizeWr  (None, 15, 15, 192)      1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 15, 15, 64)       110785    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_2  (None, 15, 15, 64)       257       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_2 (QuantizeWrap  (None, 15, 15, 64)       3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_2 (QuantizeWr  (None, 15, 15, 64)       1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 15, 15, 128)      74113     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_3  (None, 15, 15, 128)      513       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_3 (QuantizeWrap  (None, 15, 15, 128)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Quan  (None, 7, 7, 128)        1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_3 (QuantizeWr  (None, 7, 7, 128)        1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 6272)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_4 (QuantizeWr  (None, 6272)             1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               62735     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_softmax (QuantizeWrap  (None, 10)               1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 365,616\n",
            "Trainable params: 363,786\n",
            "Non-trainable params: 1,830\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "q_aware_model = quantize_model(model)\n",
        "q_aware_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYdW-VK8Q5mT"
      },
      "outputs": [],
      "source": [
        "quantize_train, quant_train_info = tfds.load(DataSet, split='train + test[:75%]', with_info=True, as_supervised=True)\n",
        "filtered_quantize_train = quantize_train.filter(lambda x, y: filter_fn(y, class_list))\n",
        "\n",
        "resized_quantize_train = prepare(filtered_quantize_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VQBS_obQ8DF",
        "outputId": "50be512f-52b8-4215-fe78-c9a43adb06b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "21/21 [==============================] - 11s 452ms/step - loss: 93.8971 - accuracy: 0.6421 - val_loss: 92.0019 - val_accuracy: 0.7373\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 5s 263ms/step - loss: 90.5158 - accuracy: 0.6736 - val_loss: 89.0145 - val_accuracy: 0.7463\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 6s 278ms/step - loss: 87.7361 - accuracy: 0.7003 - val_loss: 86.6357 - val_accuracy: 0.7483\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 5s 261ms/step - loss: 85.5085 - accuracy: 0.7067 - val_loss: 84.6382 - val_accuracy: 0.7421\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 5s 258ms/step - loss: 83.5398 - accuracy: 0.7307 - val_loss: 82.8535 - val_accuracy: 0.7334\n"
          ]
        }
      ],
      "source": [
        "resized_quantize_train = resized_quantize_train.batch(BATCH_SIZE)\n",
        "h = q_aware_model.fit(resized_quantize_train, epochs=5, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pHskuvDaRBPb",
        "outputId": "cba28a60-0b0c-4423-9c51-1c305ba7498b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFq4nw4PRDYT",
        "outputId": "66f445e2-0e16-4d64-be4b-782e60dd01b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbUyvBrRGBD"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ol3wMK2RIjN",
        "outputId": "3fe65086-275b-4d4d-bd7b-5052bff1aab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.48520710059171596\n"
          ]
        }
      ],
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVX3TFb5RKeD",
        "outputId": "9256d51d-166c-44a0-91c9-60f6c3cce62f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKEqxRdmRMOL",
        "outputId": "8341415f-5ae8-42e3-c992-341a04d73a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf2onnx==1.8.4\n",
            "  Downloading tf2onnx-1.8.4-py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.3/345.3 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (2.25.1)\n",
            "Collecting onnx>=1.4.1\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.4.1->tf2onnx==1.8.4) (4.4.0)\n",
            "Collecting protobuf<4,>=3.20.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (2.10)\n",
            "Installing collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.13.0 protobuf-3.20.3 tf2onnx-1.8.4\n",
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-02-07 09:59:43,332 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-02-07 09:59:44,202 - INFO - Signatures found in model: [serving_default].\n",
            "2023-02-07 09:59:44,202 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-02-07 09:59:44,202 - INFO - Output names: ['softmax']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2023-02-07 09:59:44,333 - WARNING - From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2023-02-07 09:59:44,378 - INFO - Using tensorflow=2.9.2, onnx=1.13.0, tf2onnx=1.8.4/cd55bf\n",
            "2023-02-07 09:59:44,378 - INFO - Using opset <onnx, 9>\n",
            "2023-02-07 09:59:44,413 - INFO - Computed 0 values for constant folding\n",
            "2023-02-07 09:59:44,553 - INFO - Optimizing ONNX model\n",
            "2023-02-07 09:59:44,604 - INFO - After optimization: BatchNormalization -4 (4->0), Cast -1 (1->0), Const -16 (27->11), Identity -10 (10->0), Transpose -20 (22->2)\n",
            "2023-02-07 09:59:44,607 - INFO - \n",
            "2023-02-07 09:59:44,607 - INFO - Successfully converted TensorFlow model /content/CadenceNet_Float/ to ONNX\n",
            "2023-02-07 09:59:44,607 - INFO - Model inputs: ['conv2d_input:0']\n",
            "2023-02-07 09:59:44,607 - INFO - Model outputs: ['softmax']\n",
            "2023-02-07 09:59:44,607 - INFO - ONNX model is saved at /content/CadenceNetOriginal_Float.onnx\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tf2onnx==1.8.4\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNetOriginal_Float.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLDe9u9sRORk",
        "outputId": "becd55e3-6ce1-4cb1-9ef8-f3451c0c29ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "381680"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quant_file = \"/content/CadenceNetOriginal_QAT.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5iITOiiRP0M",
        "outputId": "216a1801-c12f-40dc-ae18-f1030e8506d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model in Mb:  1.3894729614257812\n",
            "Quantized model in Mb:  0.3639984130859375\n",
            "Float Model Accuracy:  0.5046022353714661\n",
            "Quantized Model Accuracy:  0.48520710059171596\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Float model in Mb: \", os.path.getsize(\"/content/CadenceNetOriginal_Float.onnx\") / float(2**20))\n",
        "print(\"Quantized model in Mb: \", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float Model Accuracy: \", test_accuracy_Float)\n",
        "print(\"Quantized Model Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOfhD35IW0f",
        "outputId": "9273a3e7-b262-4d29-b9d3-7b6f53041154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnxruntime\n",
        "import onnxruntime as rt\n",
        "\n",
        "sess = rt.InferenceSession(\"/content/CadenceNetOriginal_Float.onnx\")\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "x = np.random.random((1,IMG_SIZE,IMG_SIZE,NUM_CHANNELS))\n",
        "x = x.astype(np.float32)\n",
        "res = sess.run([output_name], {input_name: x})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_byw6-0Um7c"
      },
      "outputs": [],
      "source": [
        "indices = tf.convert_to_tensor([0, 1, 2])\n",
        "depth = 3\n",
        "indic = tf.convert_to_tensor([3, 5, 8])\n",
        "tf.math.multiply(indices, indic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftfq2j6b0CrC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}