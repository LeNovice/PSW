{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeNovice/PSW/blob/main/Better_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGru5NMgz0L"
      },
      "source": [
        "Implementing\n",
        "\n",
        "Focal Loss:\n",
        "* https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/\n",
        "* https://github.com/artemmavrin/focal-loss\n",
        "\n",
        "If we make Data Augmentation as a layer in the model (maybe only for training) then we won't see overfitting on training or validation data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCeppY1fBRsz"
      },
      "outputs": [],
      "source": [
        "USE_ORIGINAL = 0\n",
        "loss = 'sparse_categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IYvbodAaO5NT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model building\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.utils as tfutils\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bJSJfW_tPA88"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "DataSet = 'caltech101'\n",
        "#'cifar10'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:(NUM_CLASSES + 1)]    #+1 because we are going to remove \"backround_google\" i.e. 4\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2fLIbNS2PDZ1"
      },
      "outputs": [],
      "source": [
        "#ds_train = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True)\n",
        "ds_train, train_info = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True, with_info = True)\n",
        "ds_test = tfds.load(DataSet, split='test', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "QOoGq7JtPGn8",
        "outputId": "e4882092-520b-4af9-ef41-55e4a87fa8b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['airplanes',\n",
              " 'bonsai',\n",
              " 'car_side',\n",
              " 'faces',\n",
              " 'faces_easy',\n",
              " 'hawksbill',\n",
              " 'ketch',\n",
              " 'leopards',\n",
              " 'motorbikes',\n",
              " 'watch']"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "if DataSet == 'caltech101':\n",
        "  class_list = [i for i in class_list if i != train_info.features['label'].str2int('background_google')]\n",
        "  class_list.sort()\n",
        "\n",
        "\"\"\"for name in train_info.features['label'].names:\n",
        "    print(name, train_info.features['label'].str2int(name))\n",
        "\"\"\"\n",
        "\n",
        "class_names = [train_info.features['label'].int2str(i) for i in class_list]\n",
        "display(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VeThcLypHU4m"
      },
      "outputs": [],
      "source": [
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApyIbKhPISb",
        "outputId": "e1288a29-3ac0-4e46-9f69-673f42db3746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 610\n",
            "9 103\n",
            "16 101\n",
            "37 337\n",
            "38 333\n",
            "46 87\n",
            "54 88\n",
            "57 155\n",
            "66 626\n",
            "95 194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8z2tJJ3PLsE",
        "outputId": "2f7331ed-dc59-4263-9192-cd0154b933a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 770\n",
            "9 98\n",
            "16 93\n",
            "37 405\n",
            "38 405\n",
            "46 70\n",
            "54 84\n",
            "57 170\n",
            "66 768\n",
            "95 209\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QdPKLGVdPNrk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "IMG_SIZE = 60\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n",
        "\n",
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)\n",
        "#Relabelling to avoid issues. Note that human readability is reduced by this\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        #values=tf.constant([tfutils.to_categorical(0, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(1, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(2, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(3, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(4, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(5, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(6, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(7, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(8, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(9, num_classes=NUM_CLASSES, dtype=np.int64)],  dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64)\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "#This function will be used in the graph execution hence @tf.function prefix\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    global loss\n",
        "    mapped_label = table.lookup(label)\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        mapped_label = tf.one_hot(indices=mapped_label, depth=NUM_CLASSES)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "#Preprocessing done as part of the graph\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#Preprocessing function which invokes above graphs\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        #f_ds = ds.filter(lambda x, y: filter_fn(y, [2,3,6]))    #[2,3,6] are the examples with lesser data. We are trying to bring back balance\n",
        "        #f_ds_aug = f_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(f_ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmLFCHD6PgLw",
        "outputId": "625f94c0-b3f3-47f7-a0c4-18f94b101ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"None_Lookup/LookupTableFindV2:0\", shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBn0xUIRPizz"
      },
      "outputs": [],
      "source": [
        "for example in resized_ds_train.take(1):\n",
        "  plt.imshow(example[0])\n",
        "  display((example[-1]))\n",
        "  display(tf.argmax(example[-1]))\n",
        "  #display(train_info.features['label'].int2str(example[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_samples_per_class_onehot(resized_ds_train, print_all=False):\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: tf.argmax(y)), int), return_counts=True)\n",
        "    else:\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    class_hist.sort()\n",
        "    return class_hist\n",
        "\n"
      ],
      "metadata": {
        "id": "uf1KScmu9odE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "avZ2eG0Ty3fi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "4115b43e-ee4e-4e3b-c78e-5fde3a19c623"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[(0, 2440),\n",
              " (1, 412),\n",
              " (2, 404),\n",
              " (3, 1348),\n",
              " (4, 1332),\n",
              " (5, 348),\n",
              " (6, 352),\n",
              " (7, 620),\n",
              " (8, 2504),\n",
              " (9, 776)]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Post prepare function, all the labels will be converted to one hot encoders. In order to get class-wise distribution, we will need to convert each one hot encoder into its label (temporarily)\n",
        "#We need a new function to handle it\n",
        "class_hist = num_samples_per_class_onehot(resized_ds_train)\n",
        "display(class_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftnZ5OyvQB98",
        "outputId": "58573172-bc5d-4fd0-a43e-286af8ec570e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Better CadenceNet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#reg = tf.keras.regularizers.L2(0.01)\n",
        "#reg = tf.keras.regularizers.L1L2(l1 =0.01, l2 = 0.1)\n",
        "reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.0)\n",
        "#beta_regularizer = 0.1\n",
        "#gamma_regularizer = 0.1\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "if USE_ORIGINAL == 1:\n",
        "\t\tdisplay(\"Original CadenceNet\")\n",
        "\t\tmodel.add(resize_and_rescale)\n",
        "\t\tmodel.add(data_augmentation)\n",
        "\t\tkernel_size = (5,5)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())                                                      #beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())\n",
        "else:\n",
        "\t\tdisplay(\"Better CadenceNet\")\n",
        "\t\tmodel.add(resize_and_rescale)\n",
        "\t\tmodel.add(data_augmentation)\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\t#kernel_size = (3,3)\n",
        "\t\t#model.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\t#model.add(layers.BatchNormalization())\n",
        "\t\t#model.add(layers.ReLU())\n",
        "\t\t#pool_size = (2,2)\n",
        "\t\t#model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\t#model.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(1000, kernel_regularizer = reg))\n",
        "\t\t#model.add(layers.Dropout(.02))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B-aANPwedhNH"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(class_hist):\n",
        "    \"\"\"\n",
        "    Returns the class weights as a tf.Tensor. Class weights are inverse of the class frequencies\n",
        "    Class frequencies are the number of samples of each class which we calculate in earlier steps\n",
        "    \"\"\"\n",
        "    inv_freq = tf.convert_to_tensor([1.0/count for label, count in class_hist], dtype=tf.float32)\n",
        "    return tfutils.normalize(inv_freq)\n",
        "\n",
        "\n",
        "def weightedloss(y_true, y_pred, gamma, class_weight):\n",
        "    \"\"\"\n",
        "    We assume that all arguments coming into this function are tf.Tensors type\n",
        "    class_weights are basically alpha in focal loss paper\n",
        "    \"\"\"\n",
        "    #ones = tf.convert_to_tensor(np.ones(shape=len(y_true)))\n",
        "    a = tf.math.multiply(tf.math.pow(tf.math.subtract(1.0, y_pred), gamma), tf.math.log(y_pred))  #((1-pt)^gamma)log(pt)\n",
        "    b = tf.math.multiply(-1.0, class_weight)                                                          #-alpha\n",
        "    b = tf.math.multiply(b,a)    \n",
        "    b = tf.math.multiply(b, y_true)\n",
        "    return b\n",
        "class WeightedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma, class_weight=np.ones(shape=NUM_CLASSES, dtype=np.float32)):\n",
        "        super().__init__()\n",
        "        self.gamma = tf.convert_to_tensor(gamma)\n",
        "        self.class_weight = tf.convert_to_tensor(class_weight, dtype=tf.float32)\n",
        "    def call(self, y_true, y_pred):\n",
        "        return weightedloss(y_true, y_pred, self.gamma, self.class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Learning_Rate = 1e-5\n",
        "#tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.0)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)"
      ],
      "metadata": {
        "id": "fHRlWtV83IeV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DHFaNgqtwZGz"
      },
      "outputs": [],
      "source": [
        "###EITHER\n",
        "\n",
        "#!pip install focal-loss\n",
        "#from focal_loss import SparseCategoricalFocalLoss \n",
        "#model.compile( optimizer = opt, loss = SparseCategoricalFocalLoss(gamma=2), metrics=['accuracy'] )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###OR\n",
        "\n",
        "model.compile( optimizer = opt, loss = loss, metrics=['accuracy'] )\n",
        "\n",
        "###OR\n",
        "#class_wts = get_class_weights(class_hist)\n",
        "#display(class_wts)\n",
        "#model.compile( optimizer = opt, loss = WeightedLoss(gamma=2.0, class_weight=class_wts), metrics=['accuracy'] )\n",
        "\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "nFFBTJNwLAcA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKoB4BcEQVkU",
        "outputId": "e308206f-8ff5-4f9b-98ab-e4b6d8b94492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     83/Unknown - 26s 275ms/step - loss: 2.1359 - accuracy: 0.2778"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (None, 60, 60, 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r83/83 [==============================] - 28s 292ms/step - loss: 2.1359 - accuracy: 0.2778 - val_loss: 2.2839 - val_accuracy: 0.0553\n",
            "Epoch 2/20\n",
            "83/83 [==============================] - 23s 283ms/step - loss: 1.7475 - accuracy: 0.3652 - val_loss: 2.2687 - val_accuracy: 0.2507\n",
            "Epoch 3/20\n",
            "83/83 [==============================] - 25s 300ms/step - loss: 1.6547 - accuracy: 0.3880 - val_loss: 2.2566 - val_accuracy: 0.2507\n",
            "Epoch 4/20\n",
            "83/83 [==============================] - 22s 271ms/step - loss: 1.6091 - accuracy: 0.4190 - val_loss: 2.2445 - val_accuracy: 0.2507\n",
            "Epoch 5/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.5797 - accuracy: 0.4420 - val_loss: 2.2201 - val_accuracy: 0.2510\n",
            "Epoch 6/20\n",
            "83/83 [==============================] - 23s 273ms/step - loss: 1.5602 - accuracy: 0.4474 - val_loss: 2.1715 - val_accuracy: 0.2520\n",
            "Epoch 7/20\n",
            "83/83 [==============================] - 22s 271ms/step - loss: 1.5375 - accuracy: 0.4496 - val_loss: 1.8771 - val_accuracy: 0.3229\n",
            "Epoch 8/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.5319 - accuracy: 0.4542 - val_loss: 1.5668 - val_accuracy: 0.4753\n",
            "Epoch 9/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.5153 - accuracy: 0.4610 - val_loss: 1.4567 - val_accuracy: 0.4531\n",
            "Epoch 10/20\n",
            "83/83 [==============================] - 23s 272ms/step - loss: 1.5110 - accuracy: 0.4646 - val_loss: 1.4243 - val_accuracy: 0.4831\n",
            "Epoch 11/20\n",
            "83/83 [==============================] - 23s 273ms/step - loss: 1.5016 - accuracy: 0.4656 - val_loss: 1.3916 - val_accuracy: 0.4990\n",
            "Epoch 12/20\n",
            "83/83 [==============================] - 22s 265ms/step - loss: 1.4914 - accuracy: 0.4704 - val_loss: 1.4921 - val_accuracy: 0.4502\n",
            "Epoch 13/20\n",
            "83/83 [==============================] - 24s 286ms/step - loss: 1.4899 - accuracy: 0.4697 - val_loss: 1.4088 - val_accuracy: 0.4899\n",
            "Epoch 14/20\n",
            "83/83 [==============================] - 23s 276ms/step - loss: 1.4841 - accuracy: 0.4708 - val_loss: 1.4557 - val_accuracy: 0.4580\n",
            "Epoch 15/20\n",
            "83/83 [==============================] - 25s 309ms/step - loss: 1.4817 - accuracy: 0.4739 - val_loss: 1.4261 - val_accuracy: 0.4635\n",
            "Epoch 16/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.4746 - accuracy: 0.4775 - val_loss: 1.3807 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "83/83 [==============================] - 22s 270ms/step - loss: 1.4686 - accuracy: 0.4776 - val_loss: 1.4173 - val_accuracy: 0.4993\n",
            "Epoch 18/20\n",
            "83/83 [==============================] - 23s 274ms/step - loss: 1.4678 - accuracy: 0.4755 - val_loss: 1.4203 - val_accuracy: 0.5075\n",
            "Epoch 19/20\n",
            "83/83 [==============================] - 23s 275ms/step - loss: 1.4675 - accuracy: 0.4772 - val_loss: 1.4434 - val_accuracy: 0.4880\n",
            "Epoch 20/20\n",
            "83/83 [==============================] - 23s 277ms/step - loss: 1.4587 - accuracy: 0.4797 - val_loss: 1.3675 - val_accuracy: 0.5260\n"
          ]
        }
      ],
      "source": [
        "#h = model.fit( resized_ds_train, epochs=10)\n",
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_test_unbatched = resized_ds_test\n",
        "resized_ds_test = resized_ds_test.batch(BATCH_SIZE)\n",
        "\n",
        "h = model.fit( resized_ds_train, epochs=20, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FuOyiBsTQkYL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "19af9f92-fc36-4385-c080-77657310496e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pxSSK5SPhnKL"
      },
      "outputs": [],
      "source": [
        "#Evaluation and confusion matrix creation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "x_test = np.asarray(list(map(lambda x: x[0], tfds.as_numpy(resized_ds_test_unbatched))))\n",
        "y_test_orig = np.asarray(list(map(lambda x: x[1], tfds.as_numpy(resized_ds_test_unbatched))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tg4EdPBuc7fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1395eccd-71c2-47ee-ce8d-2983b1dd4d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name='resizing_input'), name='resizing_input', description=\"created by layer 'resizing_input'\"), but it was called on an input with incompatible shape (32, 60, 60, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (60, 60, 3) for input KerasTensor(type_spec=TensorSpec(shape=(60, 60, 3), dtype=tf.float32, name='random_flip_input'), name='random_flip_input', description=\"created by layer 'random_flip_input'\"), but it was called on an input with incompatible shape (32, 60, 60, 3).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 1s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "sYYhORws1NnZ"
      },
      "outputs": [],
      "source": [
        "if loss!='sparse_categorical_crossentropy':\n",
        "    false_arr = np.full(shape=len(class_list), fill_value = False)\n",
        "    #y_pred = np.empty(shape=y_test_orig.shape[-1])\n",
        "    i=0\n",
        "    for i, pred in enumerate(predictions):\n",
        "        temp_arr = copy.deepcopy(false_arr)\n",
        "        np.put(temp_arr, np.argmax(pred), True)\n",
        "    if i==0:\n",
        "        y_pred = copy.deepcopy(temp_arr)\n",
        "    else:\n",
        "        y_pred = np.vstack([y_pred, temp_arr])\n",
        "    display(y_pred.shape)\n",
        "else:\n",
        "    y_pred = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(y_test_orig.shape)\n",
        "display(y_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "o-iQ19WTaE9s",
        "outputId": "0db7ab70-dee3-4d9f-fa7f-31140da5156f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3072,)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3072,)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Go8FAKdprJVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "6c8cb2c0-b25c-4e68-87c3-5d7b07163437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[584,   2,   0, 130,   6,   0,   0,   0,  42,   6],\n",
              "       [ 19,   4,   0,  43,   2,   0,   4,   0,  10,  16],\n",
              "       [ 26,   1,   0,  55,   3,   0,   1,   0,   0,   7],\n",
              "       [ 59,  13,   0, 272,  26,   1,   6,   0,   2,  26],\n",
              "       [ 66,   5,   0, 263,  30,   0,   6,   0,   1,  34],\n",
              "       [ 11,   3,   0,  44,   2,   0,   0,   0,   3,   7],\n",
              "       [ 10,   5,   0,  36,   1,   0,   5,   0,   1,  26],\n",
              "       [ 15,   1,   0, 139,   5,   0,   1,   0,   0,   9],\n",
              "       [104,   1,   0,  23,   1,   0,   1,   0, 626,  12],\n",
              "       [ 12,  21,   0,  44,   6,   1,   1,   0,  29,  95]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   airplanes       0.64      0.76      0.70       770\n",
            "      bonsai       0.07      0.04      0.05        98\n",
            "    car_side       0.00      0.00      0.00        93\n",
            "       faces       0.26      0.67      0.37       405\n",
            "  faces_easy       0.37      0.07      0.12       405\n",
            "   hawksbill       0.00      0.00      0.00        70\n",
            "       ketch       0.20      0.06      0.09        84\n",
            "    leopards       0.00      0.00      0.00       170\n",
            "  motorbikes       0.88      0.82      0.84       768\n",
            "       watch       0.40      0.45      0.43       209\n",
            "\n",
            "    accuracy                           0.53      3072\n",
            "   macro avg       0.28      0.29      0.26      3072\n",
            "weighted avg       0.50      0.53      0.48      3072\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print('Confusion Matrix')\n",
        "if loss != 'sparse_categorical_crossentropy':\n",
        "    matrix = confusion_matrix(y_test_orig.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "else:\n",
        "    matrix = confusion_matrix(y_test_orig, y_pred)\n",
        "display(matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test_orig, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FRx9tVhibX"
      },
      "source": [
        "NOT using below things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NaFdDuTQoyT"
      },
      "outputs": [],
      "source": [
        "def ret_as_numpy():\n",
        "    test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    test = prepare(test)\n",
        "    test = tfds.as_numpy(test)\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_MguzMQuZL"
      },
      "outputs": [],
      "source": [
        "test_as_np = ret_as_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWYWlODgQrFy"
      },
      "outputs": [],
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        #display(output[0])\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    #display(output[0])\n",
        "    #display(output)\n",
        "    #display(digit)\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    #display(prediction_digits)\n",
        "    #display(test_labels)\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHIU_J3QxE7"
      },
      "outputs": [],
      "source": [
        "test_accuracy_Float = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy_Float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q2Is9IY-Oo"
      },
      "source": [
        "Float checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fr6QAx7Qztb",
        "outputId": "f12b8152-ed9a-40f9-a72e-b11ebd0d7c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/238.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfG_qAU4Q3DL",
        "outputId": "a85b5a60-408f-40ec-f34c-19a0d48664a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 60, 60, 3)        3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 60, 60, 64)       4993      \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_batch_normalization (  (None, 60, 60, 64)       257       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_re_lu (QuantizeWrappe  (None, 60, 60, 64)       3         \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 30, 30, 64)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWrap  (None, 30, 30, 64)       1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 30, 30, 192)      111169    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_1  (None, 30, 30, 192)      769       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_1 (QuantizeWrap  (None, 30, 30, 192)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 15, 15, 192)      1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_1 (QuantizeWr  (None, 15, 15, 192)      1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 15, 15, 64)       110785    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_2  (None, 15, 15, 64)       257       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_2 (QuantizeWrap  (None, 15, 15, 64)       3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_2 (QuantizeWr  (None, 15, 15, 64)       1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 15, 15, 128)      74113     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_3  (None, 15, 15, 128)      513       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_re_lu_3 (QuantizeWrap  (None, 15, 15, 128)      3         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Quan  (None, 7, 7, 128)        1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_dropout_3 (QuantizeWr  (None, 7, 7, 128)        1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 6272)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout_4 (QuantizeWr  (None, 6272)             1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               62735     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_softmax (QuantizeWrap  (None, 10)               1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 365,616\n",
            "Trainable params: 363,786\n",
            "Non-trainable params: 1,830\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "q_aware_model = quantize_model(model)\n",
        "q_aware_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYdW-VK8Q5mT"
      },
      "outputs": [],
      "source": [
        "quantize_train, quant_train_info = tfds.load(DataSet, split='train + test[:75%]', with_info=True, as_supervised=True)\n",
        "filtered_quantize_train = quantize_train.filter(lambda x, y: filter_fn(y, class_list))\n",
        "\n",
        "resized_quantize_train = prepare(filtered_quantize_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VQBS_obQ8DF",
        "outputId": "50be512f-52b8-4215-fe78-c9a43adb06b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "21/21 [==============================] - 11s 452ms/step - loss: 93.8971 - accuracy: 0.6421 - val_loss: 92.0019 - val_accuracy: 0.7373\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 5s 263ms/step - loss: 90.5158 - accuracy: 0.6736 - val_loss: 89.0145 - val_accuracy: 0.7463\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 6s 278ms/step - loss: 87.7361 - accuracy: 0.7003 - val_loss: 86.6357 - val_accuracy: 0.7483\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 5s 261ms/step - loss: 85.5085 - accuracy: 0.7067 - val_loss: 84.6382 - val_accuracy: 0.7421\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 5s 258ms/step - loss: 83.5398 - accuracy: 0.7307 - val_loss: 82.8535 - val_accuracy: 0.7334\n"
          ]
        }
      ],
      "source": [
        "resized_quantize_train = resized_quantize_train.batch(BATCH_SIZE)\n",
        "h = q_aware_model.fit(resized_quantize_train, epochs=5, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pHskuvDaRBPb",
        "outputId": "cba28a60-0b0c-4423-9c51-1c305ba7498b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFq4nw4PRDYT",
        "outputId": "66f445e2-0e16-4d64-be4b-782e60dd01b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbUyvBrRGBD"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ol3wMK2RIjN",
        "outputId": "3fe65086-275b-4d4d-bd7b-5052bff1aab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.48520710059171596\n"
          ]
        }
      ],
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVX3TFb5RKeD",
        "outputId": "9256d51d-166c-44a0-91c9-60f6c3cce62f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKEqxRdmRMOL",
        "outputId": "8341415f-5ae8-42e3-c992-341a04d73a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf2onnx==1.8.4\n",
            "  Downloading tf2onnx-1.8.4-py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m345.3/345.3 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (2.25.1)\n",
            "Collecting onnx>=1.4.1\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from tf2onnx==1.8.4) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.4.1->tf2onnx==1.8.4) (4.4.0)\n",
            "Collecting protobuf<4,>=3.20.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx==1.8.4) (2.10)\n",
            "Installing collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.13.0 protobuf-3.20.3 tf2onnx-1.8.4\n",
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-02-07 09:59:43,332 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-02-07 09:59:44,202 - INFO - Signatures found in model: [serving_default].\n",
            "2023-02-07 09:59:44,202 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-02-07 09:59:44,202 - INFO - Output names: ['softmax']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2023-02-07 09:59:44,333 - WARNING - From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2023-02-07 09:59:44,378 - INFO - Using tensorflow=2.9.2, onnx=1.13.0, tf2onnx=1.8.4/cd55bf\n",
            "2023-02-07 09:59:44,378 - INFO - Using opset <onnx, 9>\n",
            "2023-02-07 09:59:44,413 - INFO - Computed 0 values for constant folding\n",
            "2023-02-07 09:59:44,553 - INFO - Optimizing ONNX model\n",
            "2023-02-07 09:59:44,604 - INFO - After optimization: BatchNormalization -4 (4->0), Cast -1 (1->0), Const -16 (27->11), Identity -10 (10->0), Transpose -20 (22->2)\n",
            "2023-02-07 09:59:44,607 - INFO - \n",
            "2023-02-07 09:59:44,607 - INFO - Successfully converted TensorFlow model /content/CadenceNet_Float/ to ONNX\n",
            "2023-02-07 09:59:44,607 - INFO - Model inputs: ['conv2d_input:0']\n",
            "2023-02-07 09:59:44,607 - INFO - Model outputs: ['softmax']\n",
            "2023-02-07 09:59:44,607 - INFO - ONNX model is saved at /content/CadenceNetOriginal_Float.onnx\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tf2onnx==1.8.4\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNetOriginal_Float.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLDe9u9sRORk",
        "outputId": "becd55e3-6ce1-4cb1-9ef8-f3451c0c29ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "381680"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "quant_file = \"/content/CadenceNetOriginal_QAT.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5iITOiiRP0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216a1801-c12f-40dc-ae18-f1030e8506d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model in Mb:  1.3894729614257812\n",
            "Quantized model in Mb:  0.3639984130859375\n",
            "Float Model Accuracy:  0.5046022353714661\n",
            "Quantized Model Accuracy:  0.48520710059171596\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Float model in Mb: \", os.path.getsize(\"/content/CadenceNetOriginal_Float.onnx\") / float(2**20))\n",
        "print(\"Quantized model in Mb: \", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float Model Accuracy: \", test_accuracy_Float)\n",
        "print(\"Quantized Model Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime\n",
        "import onnxruntime as rt\n",
        "\n",
        "sess = rt.InferenceSession(\"/content/CadenceNetOriginal_Float.onnx\")\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "x = np.random.random((1,IMG_SIZE,IMG_SIZE,NUM_CHANNELS))\n",
        "x = x.astype(np.float32)\n",
        "res = sess.run([output_name], {input_name: x})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOfhD35IW0f",
        "outputId": "9273a3e7-b262-4d29-b9d3-7b6f53041154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_byw6-0Um7c"
      },
      "outputs": [],
      "source": [
        "indices = tf.convert_to_tensor([0, 1, 2])\n",
        "depth = 3\n",
        "indic = tf.convert_to_tensor([3, 5, 8])\n",
        "tf.math.multiply(indices, indic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ftfq2j6b0CrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}