{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtuzdD-Y0hx9"
      },
      "source": [
        "TODO: Refer\n",
        "\n",
        "https://debuggercafe.com/getting-95-accuracy-on-the-caltech101-dataset-using-deep-learning/\n",
        "\n",
        "Data Pipeline:\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#as_numpy_iterator\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPUNbkOBLRGr"
      },
      "source": [
        "Loading the Caltech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q-XcSlHRLMMf"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model buildingZ\n",
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to get the number of samples per class"
      ],
      "metadata": {
        "id": "Q9PJJVQvHaon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially we will only try to train for 10 classes.\n",
        "\n",
        "https://github.com/tensorflow/datasets/issues/1923#issuecomment-1361608072"
      ],
      "metadata": {
        "id": "Habr8raqQY0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 10\n",
        "DataSet = 'caltech101'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        #class_hist[val] = count\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:NUM_CLASSES]\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ],
      "metadata": {
        "id": "uriRoxiVHaIa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4DETpDXuOIHT"
      },
      "outputs": [],
      "source": [
        "(ds, ds_info) = tfds.load(DataSet, with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ds_train, train_info = ds[\"train\"], ds_info.splits['train']\n",
        "#ds_test, test_info = ds[\"test\"], ds_info.splits['test']\n",
        "\n",
        "ds_train, train_info = tfds.load(DataSet, split='train + test[:75%]', with_info=True, as_supervised=True)\n",
        "ds_test, test_info = tfds.load(DataSet, split='test[25%:]', with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "x3EBZDwjUBN9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "display(class_list)\n",
        "class_list.sort()\n",
        "display(class_list)\n",
        "filtered_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "filtered_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "zvgWBdGLTj4p",
        "outputId": "357812fd-ab3f-4dc9-ff56-ef60daac60c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[66, 1, 4, 38, 37, 95, 57, 9, 16, 54]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 37, 38, 54, 57, 66, 95]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples_per_class(filtered_ds_train, print_all=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oCiprcJHMHO",
        "outputId": "e9468827-e2b5-4bed-c5ea-44b4fc09652e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 608\n",
            "4 355\n",
            "9 106\n",
            "16 104\n",
            "37 335\n",
            "38 336\n",
            "54 91\n",
            "57 152\n",
            "66 621\n",
            "95 197\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples_per_class(filtered_ds_test, print_all=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj2yPwB6Tqnv",
        "outputId": "e01dd4ac-378a-49a3-923f-bba4ca49109d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 581\n",
            "4 324\n",
            "9 72\n",
            "16 67\n",
            "37 310\n",
            "38 296\n",
            "54 63\n",
            "57 136\n",
            "66 568\n",
            "95 151\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prTD3lYBb1jk"
      },
      "source": [
        "Data Preprocessing\n",
        "\n",
        "We could use adapt() methods to get normlazation (feature wise) parameters. https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method\n",
        "\n",
        "https://stackoverflow.com/questions/57657386/tensorflow-datasets-reshape-images\n",
        "\n",
        "\n",
        "MAINLY:\n",
        "https://www.tensorflow.org/datasets/keras_example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "IMG_SIZE = 224\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n"
      ],
      "metadata": {
        "id": "24ab_2wmhw5X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuc6xq51_uK6"
      },
      "source": [
        "Resizing and re-scaling images to a given dataset.\n",
        "Tutorial used: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "\n",
        "For data pipeline you may also refer to\n",
        "https://github.com/tensorflow/datasets/issues/720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZVWbmo7eS-N2"
      },
      "outputs": [],
      "source": [
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64),\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    mapped_label = table.lookup(label)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "#buffer_size = ds_info.splits['train'].num_examples      #Might return -2   https://stackoverflow.com/questions/50737192/tf-data-dataset-how-to-get-the-dataset-size-number-of-elements-in-an-epoch\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#resized_ds_train = filtered_ds_train.map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#https://www.tensorflow.org/tutorials/images/data_augmentation#apply_the_preprocessing_layers_to_the_datasets\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        #f_ds = ds.filter(lambda x, y: filter_fn(y, [2,3,6]))    #[2,3,6] are the examples with lesser data. We are trying to bring back balance\n",
        "        #f_ds_aug = f_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(f_ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resized_ds_train = prepare(filtered_ds_train, augment=True)\n",
        "resized_ds_test = prepare(filtered_ds_test)"
      ],
      "metadata": {
        "id": "3PxROybzWse8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568ad04c-9b9e-4b5c-94a4-93f8b46cbeeb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"None_Lookup/LookupTableFindV2:0\", shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZX5Sjv1T2JQ",
        "outputId": "8280d9e5-6e0e-49ee-e85d-0228079909e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2432\n",
            "1 1420\n",
            "2 424\n",
            "3 416\n",
            "4 1340\n",
            "5 1344\n",
            "6 364\n",
            "7 608\n",
            "8 2484\n",
            "9 788\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQG4ddBvT21n",
        "outputId": "3ac4c90e-7dec-4602-84c8-a291685cfd7d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 581\n",
            "1 324\n",
            "2 72\n",
            "3 67\n",
            "4 310\n",
            "5 296\n",
            "6 63\n",
            "7 136\n",
            "8 568\n",
            "9 151\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtQ38QKWw53M"
      },
      "source": [
        "display few examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = tfds.show_examples(filtered_ds_train, ds_info)"
      ],
      "metadata": {
        "id": "TcBi8KWv10Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeaeP_-bTf4A"
      },
      "outputs": [],
      "source": [
        "#ds_example = ds_train.take(10)\n",
        "fig = tfds.show_examples(resized_ds_train, ds_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSpCep86OIfT"
      },
      "source": [
        "Prepare the model\n",
        "For Batchnorm, refer https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
        "\n",
        "Here they say that During training, the layer normalizes its output using the mean and standard deviation of the **current batch** of inputs.\n",
        "\n",
        "In order to make BatchNorm great, should we be using a larger batch as input?\n",
        "\n",
        "However, during Inference mode, the mean ans tsd deviation does not correspond to the current batch. Rather it is a moving mean and std dev of all the bacthes seen in training phase. (Thus, the parameters in inference phase for batch norm do not change)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)"
      ],
      "metadata": {
        "id": "YuHhwzDITyz4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Mg41t1e5S8XA"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "#model.add(resize_and_rescale)\n",
        "\n",
        "kernel_size = (5,5)\n",
        "model.add(layers.Conv2D(64, kernel_size, input_shape = input_shape))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "kernel_size = (3,3)\n",
        "model.add(layers.Conv2D(192, kernel_size))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "kernel_size = (3,3)\n",
        "model.add(layers.Conv2D(64, kernel_size))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "kernel_size = (3,3)\n",
        "model.add(layers.Conv2D(128, kernel_size))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s-PFEc55hxNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639c5ffa-c960-46b0-ce95-079afeaae023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 220, 220, 64)      4864      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 220, 220, 64)     256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 110, 110, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 108, 108, 192)     110784    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 108, 108, 192)    768       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 54, 54, 192)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 52, 52, 64)        110656    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 52, 52, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 26, 26, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 24, 24, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 12, 12, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 18432)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                184330    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 486,282\n",
            "Trainable params: 485,386\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Learning_Rate = 1e-1                                            #https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.0)\n",
        "model.compile( optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh35uw-5keV2"
      },
      "source": [
        "Reference: https://github.com/tensorflow/datasets/issues/720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yUisIOtOiKHV"
      },
      "outputs": [],
      "source": [
        "#resized_ds_train = resized_ds_train.cache()\n",
        "\n",
        "#resized_ds_train = resized_ds_train.shuffle(buffer_size)\n",
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_test = resized_ds_test.batch(BATCH_SIZE)\n",
        "#resized_ds_train = resized_ds_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "20Xb1Xb65V6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = model.fit( resized_ds_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFL849PI32M3",
        "outputId": "0fd4ddab-2314-4f10-beca-78fdec74e747"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "91/91 [==============================] - 99s 1s/step - loss: 52.3428 - accuracy: 0.4900\n",
            "Epoch 2/10\n",
            "91/91 [==============================] - 91s 1s/step - loss: 6.5554 - accuracy: 0.5978\n",
            "Epoch 3/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 2.4293 - accuracy: 0.6568\n",
            "Epoch 4/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 2.9254 - accuracy: 0.6558\n",
            "Epoch 5/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 1.7682 - accuracy: 0.7009\n",
            "Epoch 6/10\n",
            "91/91 [==============================] - 91s 1s/step - loss: 1.3828 - accuracy: 0.7151\n",
            "Epoch 7/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 1.3924 - accuracy: 0.7247\n",
            "Epoch 8/10\n",
            "91/91 [==============================] - 99s 1s/step - loss: 1.2556 - accuracy: 0.7376\n",
            "Epoch 9/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 1.4346 - accuracy: 0.7416\n",
            "Epoch 10/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 1.4073 - accuracy: 0.7457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,acc = model.evaluate(resized_ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAslbj3d5T8_",
        "outputId": "76c14dbd-935e-44c5-d542-3c06b20edd7d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 5s 238ms/step - loss: 0.9108 - accuracy: 0.8107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model trained. Now we will create float as well as quantized model"
      ],
      "metadata": {
        "id": "eDw2jJ1gvvIi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAOVonnhv1aR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0Bq1xbqN8wztRwq7vScxE"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}