{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeNovice/PSW/blob/main/Evaluation_Main_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtuzdD-Y0hx9"
      },
      "source": [
        "TODO: Refer\n",
        "\n",
        "https://debuggercafe.com/getting-95-accuracy-on-the-caltech101-dataset-using-deep-learning/\n",
        "\n",
        "Data Pipeline:\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#as_numpy_iterator\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPUNbkOBLRGr"
      },
      "source": [
        "Loading the Caltech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-XcSlHRLMMf"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model buildingZ\n",
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9PJJVQvHaon"
      },
      "source": [
        "Code to get the number of samples per class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Habr8raqQY0C"
      },
      "source": [
        "Initially we will only try to train for 10 classes.\n",
        "\n",
        "https://github.com/tensorflow/datasets/issues/1923#issuecomment-1361608072"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uriRoxiVHaIa"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "DataSet = 'caltech101'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        #class_hist[val] = count\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:NUM_CLASSES]\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DETpDXuOIHT"
      },
      "outputs": [],
      "source": [
        "#(ds, ds_info) = tfds.load(DataSet, with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3EBZDwjUBN9"
      },
      "outputs": [],
      "source": [
        "#ds_train, train_info = ds[\"train\"], ds_info.splits['train']\n",
        "#ds_test, test_info = ds[\"test\"], ds_info.splits['test']\n",
        "\n",
        "ds_train = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True)\n",
        "ds_test = tfds.load(DataSet, split='test', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvgWBdGLTj4p"
      },
      "outputs": [],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "class_list.sort()\n",
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oCiprcJHMHO",
        "outputId": "1b9bf371-c8d0-4d25-85d3-a5eb328171ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 610\n",
            "4 358\n",
            "9 103\n",
            "16 100\n",
            "37 336\n",
            "38 334\n",
            "54 89\n",
            "57 151\n",
            "66 625\n",
            "95 193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj2yPwB6Tqnv",
        "outputId": "855bb621-9e1f-4566-df80-00635125a9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 770\n",
            "4 437\n",
            "9 98\n",
            "16 93\n",
            "37 405\n",
            "38 405\n",
            "54 84\n",
            "57 170\n",
            "66 768\n",
            "95 209\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prTD3lYBb1jk"
      },
      "source": [
        "Data Preprocessing\n",
        "\n",
        "We could use adapt() methods to get normlazation (feature wise) parameters. https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method\n",
        "\n",
        "https://stackoverflow.com/questions/57657386/tensorflow-datasets-reshape-images\n",
        "\n",
        "\n",
        "MAINLY:\n",
        "https://www.tensorflow.org/datasets/keras_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24ab_2wmhw5X"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "IMG_SIZE = 224\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuc6xq51_uK6"
      },
      "source": [
        "Resizing and re-scaling images to a given dataset.\n",
        "Tutorial used: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "\n",
        "For data pipeline you may also refer to\n",
        "https://github.com/tensorflow/datasets/issues/720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVWbmo7eS-N2"
      },
      "outputs": [],
      "source": [
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64),\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    mapped_label = table.lookup(label)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "#buffer_size = ds_info.splits['train'].num_examples      #Might return -2   https://stackoverflow.com/questions/50737192/tf-data-dataset-how-to-get-the-dataset-size-number-of-elements-in-an-epoch\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#resized_ds_train = filtered_ds_train.map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#https://www.tensorflow.org/tutorials/images/data_augmentation#apply_the_preprocessing_layers_to_the_datasets\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        #f_ds = ds.filter(lambda x, y: filter_fn(y, [2,3,6]))    #[2,3,6] are the examples with lesser data. We are trying to bring back balance\n",
        "        #f_ds_aug = f_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(f_ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxROybzWse8",
        "outputId": "2215f7b7-98b8-4685-9936-3e779b9d54ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"None_Lookup/LookupTableFindV2:0\", shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZX5Sjv1T2JQ",
        "outputId": "de71b187-e11a-43d5-ed88-283263c85926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2440\n",
            "1 1432\n",
            "2 412\n",
            "3 400\n",
            "4 1344\n",
            "5 1336\n",
            "6 356\n",
            "7 604\n",
            "8 2500\n",
            "9 772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQG4ddBvT21n",
        "outputId": "5ddf3db7-a7f6-426c-c9ad-53a213dc7188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 770\n",
            "1 437\n",
            "2 98\n",
            "3 93\n",
            "4 405\n",
            "5 405\n",
            "6 84\n",
            "7 170\n",
            "8 768\n",
            "9 209\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSpCep86OIfT"
      },
      "source": [
        "Prepare the model\n",
        "For Batchnorm, refer https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
        "\n",
        "Here they say that During training, the layer normalizes its output using the mean and standard deviation of the **current batch** of inputs.\n",
        "\n",
        "In order to make BatchNorm great, should we be using a larger batch as input?\n",
        "\n",
        "However, during Inference mode, the mean ans tsd deviation does not correspond to the current batch. Rather it is a moving mean and std dev of all the bacthes seen in training phase. (Thus, the parameters in inference phase for batch norm do not change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuHhwzDITyz4"
      },
      "outputs": [],
      "source": [
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg41t1e5S8XA"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "#model.add(resize_and_rescale)\n",
        "\n",
        "kernel_size = (5,5)\n",
        "model.add(layers.Conv2D(64, kernel_size, input_shape = input_shape))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "kernel_size = (3,3)\n",
        "model.add(layers.Conv2D(192, kernel_size))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "kernel_size = (3,3)\n",
        "model.add(layers.Conv2D(64, kernel_size))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "kernel_size = (3,3)\n",
        "model.add(layers.Conv2D(128, kernel_size))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "model.add(layers.BatchNormalization())\n",
        "pool_size = (2,2)\n",
        "model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-PFEc55hxNm",
        "outputId": "f1c8abce-574c-43e6-ef20-e84e6ba5d864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 220, 220, 64)      4864      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 220, 220, 64)     256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 110, 110, 64)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 108, 108, 192)     110784    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 108, 108, 192)    768       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 54, 54, 192)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 64)        110656    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 52, 52, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 24, 24, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 12, 12, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                184330    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 486,282\n",
            "Trainable params: 485,386\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Learning_Rate = 1e-3                                            #https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.0)\n",
        "#model.compile( optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
        "model.compile( optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['sparse_categorical_crossentropy'] )\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh35uw-5keV2"
      },
      "source": [
        "Reference: https://github.com/tensorflow/datasets/issues/720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUisIOtOiKHV"
      },
      "outputs": [],
      "source": [
        "#resized_ds_train = resized_ds_train.cache()\n",
        "\n",
        "#resized_ds_train = resized_ds_train.shuffle(buffer_size)\n",
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_test = resized_ds_test.batch(BATCH_SIZE)\n",
        "#resized_ds_train = resized_ds_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Xb1Xb65V6N"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFL849PI32M3",
        "outputId": "6151d6f4-ff45-4570-d6ae-95221472be41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "91/91 [==============================] - 110s 1s/step - loss: 1.6768 - sparse_categorical_crossentropy: 1.6768\n",
            "Epoch 2/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 1.1130 - sparse_categorical_crossentropy: 1.1130\n",
            "Epoch 3/10\n",
            "91/91 [==============================] - 93s 1s/step - loss: 0.9812 - sparse_categorical_crossentropy: 0.9812\n",
            "Epoch 4/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 0.8818 - sparse_categorical_crossentropy: 0.8818\n",
            "Epoch 5/10\n",
            "91/91 [==============================] - 93s 1s/step - loss: 0.7394 - sparse_categorical_crossentropy: 0.7394\n",
            "Epoch 6/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 0.6423 - sparse_categorical_crossentropy: 0.6423\n",
            "Epoch 7/10\n",
            "91/91 [==============================] - 93s 1s/step - loss: 0.5766 - sparse_categorical_crossentropy: 0.5766\n",
            "Epoch 8/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 0.5202 - sparse_categorical_crossentropy: 0.5202\n",
            "Epoch 9/10\n",
            "91/91 [==============================] - 93s 1s/step - loss: 0.4979 - sparse_categorical_crossentropy: 0.4979\n",
            "Epoch 10/10\n",
            "91/91 [==============================] - 92s 1s/step - loss: 0.4799 - sparse_categorical_crossentropy: 0.4799\n"
          ]
        }
      ],
      "source": [
        "h = model.fit( resized_ds_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAslbj3d5T8_",
        "outputId": "76fe1e05-c1e3-4a3f-bafc-3bb0b0400e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27/27 [==============================] - 10s 350ms/step - loss: 2.8059 - sparse_categorical_crossentropy: 2.8059\n"
          ]
        }
      ],
      "source": [
        "loss,acc = model.evaluate(resized_ds_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDw2jJ1gvvIi"
      },
      "source": [
        "Model trained. Now we will create float as well as quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PkqxcV8yL8A"
      },
      "outputs": [],
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAOVonnhv1aR",
        "outputId": "6512f869-8516-4d2e-f1d9-efb645e7be2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 224, 224, 3)      3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 220, 220, 64)     4993      \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_batch_normalization (  (None, 220, 220, 64)     259       \n",
            " QuantizeWrapperV2)                                              \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 110, 110, 64)     1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 108, 108, 192)    111169    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_1  (None, 108, 108, 192)    771       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 54, 54, 192)      1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 52, 52, 64)       110785    \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_2  (None, 52, 52, 64)       259       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Quan  (None, 26, 26, 64)       1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 24, 24, 128)      74113     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_batch_normalization_3  (None, 24, 24, 128)      515       \n",
            "  (QuantizeWrapperV2)                                            \n",
            "                                                                 \n",
            " quant_max_pooling2d_3 (Quan  (None, 12, 12, 128)      1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 18432)            1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               184335    \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 487,207\n",
            "Trainable params: 485,386\n",
            "Non-trainable params: 1,821\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "q_aware_model = quantize_model(model)\n",
        "#q_aware_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "q_aware_model.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['sparse_categorical_crossentropy'])\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttJjDT4fxypo"
      },
      "outputs": [],
      "source": [
        "quantize_train, quant_train_info = tfds.load(DataSet, split='train + test[:75%]', with_info=True, as_supervised=True)\n",
        "filtered_quantize_train = quantize_train.filter(lambda x, y: filter_fn(y, class_list))\n",
        "#resized_quantize_train = prepare(filtered_quantize_train, resize_only=True)\n",
        "resized_quantize_train = prepare(filtered_quantize_train)\n",
        "\n",
        "#fig = tfds.show_examples(resized_quantize_train, ds_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au1jzJfrBxNO",
        "outputId": "b4399e8a-8c1a-4310-bb3e-230330ad73c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "23/23 [==============================] - 25s 1s/step - loss: 1.5907 - sparse_categorical_crossentropy: 1.5907\n",
            "Epoch 2/5\n",
            "23/23 [==============================] - 19s 811ms/step - loss: 1.2884 - sparse_categorical_crossentropy: 1.2884\n",
            "Epoch 3/5\n",
            "23/23 [==============================] - 19s 809ms/step - loss: 1.4193 - sparse_categorical_crossentropy: 1.4193\n",
            "Epoch 4/5\n",
            "23/23 [==============================] - 19s 805ms/step - loss: 1.5287 - sparse_categorical_crossentropy: 1.5287\n",
            "Epoch 5/5\n",
            "23/23 [==============================] - 19s 815ms/step - loss: 1.0931 - sparse_categorical_crossentropy: 1.0931\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1c0177910>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "resized_quantize_train = resized_quantize_train.batch(BATCH_SIZE)\n",
        "q_aware_model.fit(resized_quantize_train, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ5TWSxVzIIL",
        "outputId": "0f195e19-3195-421b-b2a9-133d9b1203e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses while saving (showing 5 of 16). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6k1Pdc-2xmX"
      },
      "source": [
        "Save the Float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cna44KG62VL3",
        "outputId": "305aea8d-c4c4-4347-908d-30805002affc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za8NOpnC3P7p",
        "outputId": "33c12dae-4f70-4230-8708-798df3bd9393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.8/dist-packages (1.13.0)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from tf2onnx) (1.13.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.8/dist-packages (from tf2onnx) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from tf2onnx) (2.25.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tf2onnx) (1.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.4.1->tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.4.1->tf2onnx) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->tf2onnx) (2022.12.7)\n",
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-01-27 12:57:37,232 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-01-27 12:57:37,696 - INFO - Signatures found in model: [serving_default].\n",
            "2023-01-27 12:57:37,696 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-01-27 12:57:37,697 - INFO - Output names: ['dense']\n",
            "2023-01-27 12:57:37,856 - INFO - Using tensorflow=2.9.2, onnx=1.13.0, tf2onnx=1.13.0/2c1db5\n",
            "2023-01-27 12:57:37,856 - INFO - Using opset <onnx, 13>\n",
            "2023-01-27 12:57:37,976 - INFO - Computed 0 values for constant folding\n",
            "2023-01-27 12:57:38,066 - INFO - Optimizing ONNX model\n",
            "2023-01-27 12:57:38,137 - INFO - After optimization: BatchNormalization -4 (4->0), Cast -1 (1->0), Const -16 (27->11), Identity -2 (2->0), Transpose -22 (24->2)\n",
            "2023-01-27 12:57:38,142 - INFO - \n",
            "2023-01-27 12:57:38,142 - INFO - Successfully converted TensorFlow model /content/CadenceNet_Float/ to ONNX\n",
            "2023-01-27 12:57:38,143 - INFO - Model inputs: ['conv2d_input']\n",
            "2023-01-27 12:57:38,143 - INFO - Model outputs: ['dense']\n",
            "2023-01-27 12:57:38,143 - INFO - ONNX model is saved at /content/CadenceNet_Float_sparseCrossEntropy.onnx\n"
          ]
        }
      ],
      "source": [
        "#convert to onnx\n",
        "!pip install -U tf2onnx\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNet_Float_sparseCrossEntropy.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-KO0PW93dB3"
      },
      "source": [
        "Save QAT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N_Z-Yev3e4o",
        "outputId": "ea1986d8-b0cb-4b0a-f115-f97628095369"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "503256"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "quant_file = \"/content/quantized_model_sparseCrossEntropy_Rescaled.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AkK4auuayiV"
      },
      "source": [
        "Define evaluator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ret_as_numpy():\n",
        "    test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    test = prepare(test)\n",
        "    test = tfds.as_numpy(test)\n",
        "    return test"
      ],
      "metadata": {
        "id": "wyxm51ZxB2iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5FUhkwN3hQ_"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(test_example[-1])\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGd2iDPuRWZ8"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fj2J-4RRYvd",
        "outputId": "128ddc66-3b0e-4f41-9bc4-b554d480a250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.47879684418145957\n"
          ]
        }
      ],
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_as_np = ret_as_numpy()\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(test_example[-1])\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        #display(output[0])\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    display(prediction_digits)\n",
        "    display(test_labels)\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "hzBBnzSSBWI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CRXaWhGgnLV8",
        "outputId": "f92680aa-89da-454f-8624-b9cc8100cb15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 5,\n",
              " 1,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 2,\n",
              " 9,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 8,\n",
              " 6,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 8,\n",
              " 9,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 6,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 2,\n",
              " 2,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 7,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 9,\n",
              " 9,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 9,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 5,\n",
              " 7,\n",
              " 7,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 7,\n",
              " 5,\n",
              " 4,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 9,\n",
              " 8,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 7,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 5,\n",
              " 8,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 7,\n",
              " 7,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 9,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 4,\n",
              " 8,\n",
              " 9,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 8,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 5,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 8,\n",
              " 5,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 9,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 2,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 7,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 8,\n",
              " 9,\n",
              " 6,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 6,\n",
              " 7,\n",
              " 1,\n",
              " 4,\n",
              " 8,\n",
              " 7,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 5,\n",
              " 7,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 7,\n",
              " 8,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 9,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 9,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 9,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 7,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 7,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 4,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 4,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 2,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 7,\n",
              " 7,\n",
              " 2,\n",
              " 8,\n",
              " 8,\n",
              " 9,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 5,\n",
              " 9,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 5,\n",
              " 8,\n",
              " 4,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 7,\n",
              " 8,\n",
              " 8,\n",
              " 2,\n",
              " 9,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 2,\n",
              " 2,\n",
              " 9,\n",
              " 8,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 7,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 9,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 7,\n",
              " 5,\n",
              " 7,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 4,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 2,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 9,\n",
              " 5,\n",
              " ...]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 5,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 5,\n",
              " 9,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 7,\n",
              " 0,\n",
              " 3,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 2,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 3,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 5,\n",
              " 4,\n",
              " 8,\n",
              " 8,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 8,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 9,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 4,\n",
              " 9,\n",
              " 9,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 8,\n",
              " 0,\n",
              " 9,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 6,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 2,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 6,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 6,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 5,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 4,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 1,\n",
              " 9,\n",
              " 4,\n",
              " 7,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 5,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 5,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 6,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 4,\n",
              " 3,\n",
              " 9,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 8,\n",
              " 7,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 9,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 9,\n",
              " 6,\n",
              " 8,\n",
              " 6,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 6,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 3,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 5,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 2,\n",
              " 4,\n",
              " 4,\n",
              " 5,\n",
              " 3,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 9,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 0,\n",
              " 7,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 6,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 5,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 6,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 9,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 5,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 2,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 4,\n",
              " 9,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 4,\n",
              " 9,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 9,\n",
              " 8,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 6,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 7,\n",
              " 5,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 9,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " ...]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float test_accuracy: 0.3487836949375411\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PL_9KH5oLzV2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}